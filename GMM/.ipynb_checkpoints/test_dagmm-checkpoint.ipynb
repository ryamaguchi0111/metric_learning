{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    A = np.array([[2.,0.]    # 行列Aの生成\n",
    "                 ,[0.,5.]])\n",
    "    L = np.linalg.cholesky(A)      # 行列AをQR分解\n",
    "    # 結果を表示\n",
    "    print(\"A=\\n\", A)\n",
    "    print(\"L=\\n\", L)\n",
    "    print(\"L*L^T=\\n\", L.dot(L.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      " [[2. 0.]\n",
      " [0. 5.]]\n",
      "L=\n",
      " [[1.41421356 0.        ]\n",
      " [0.         2.23606798]]\n",
      "L*L^T=\n",
      " [[2. 0.]\n",
      " [0. 5.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e-08])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "from utils import *\n",
    "\n",
    "# eps = np.array([1.e-8])\n",
    "eps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n",
    "print(eps)\n",
    "\n",
    "\n",
    "class Cholesky(torch.autograd.Function):\n",
    "    def forward(ctx, a):\n",
    "        l = torch.cholesky(a, upper=False)\n",
    "#         l = torch.potrf(a, False)\n",
    "        ctx.save_for_backward(l)\n",
    "        return l\n",
    "    def backward(ctx, grad_output):\n",
    "        l, = ctx.saved_variables\n",
    "        linv = l.inverse()\n",
    "        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
    "            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
    "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "        return s\n",
    "\n",
    "    \n",
    "######################################\n",
    "#   Compression Network\n",
    "######################################\n",
    "class CompressionNetworkArrhythmia(nn.Module):\n",
    "    \"\"\"Defines a compression network for the Arrhythmia dataset as described in\n",
    "    the paper.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Linear(32, 10),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(10, 2))\n",
    "        self.decoder = nn.Sequential(nn.Linear(2, 10),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(10, 32))\n",
    "\n",
    "        self._reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.encoder(input)\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def encode(self,  input):\n",
    "        return self.encoder(input)\n",
    "\n",
    "    def decode(self, input):\n",
    "        return self.decoder(input)\n",
    "\n",
    "    def reconstruction_loss(self, input, target):\n",
    "        target_hat = self(input)\n",
    "        return self._reconstruction_loss(target_hat, target)\n",
    "\n",
    "######################################\n",
    "#   Estimation Network\n",
    "######################################\n",
    "class EstimationNetworkArrhythmia(nn.Module):\n",
    "    \"\"\"Defines a estimation network for the Arrhythmia dataset as described in\n",
    "    the paper.\"\"\"\n",
    "    def __init__(self, n_gmm=2, latent_dim=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(4, 10),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Dropout(p=0.5),\n",
    "                                 nn.Linear(10, 2),\n",
    "                                 nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "    \n",
    "\n",
    "######################################\n",
    "#   DAGMM\n",
    "######################################\n",
    "class DAGMM(nn.Module):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "    def __init__(self, n_gmm = 2, latent_dim=4):\n",
    "        super(DAGMM, self).__init__()\n",
    "        \n",
    "        self.eps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n",
    "        compressor = CompressionNetworkArrhythmia()\n",
    "        estimator = EstimationNetworkArrhythmia(n_gmm, latent_dim)\n",
    "        \n",
    "#         layers = []\n",
    "#         layers += [nn.Linear(118,60)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(60,30)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(30,10)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(10,1)]\n",
    "\n",
    "        self.encoder = compressor.encoder\n",
    "\n",
    "\n",
    "#         layers = []\n",
    "#         layers += [nn.Linear(1,10)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(10,30)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(30,60)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Linear(60,118)]\n",
    "\n",
    "        self.decoder = compressor.decoder\n",
    "    \n",
    "#         layers = []\n",
    "#         layers += [nn.Linear(latent_dim,10)]\n",
    "#         layers += [nn.Tanh()]        \n",
    "#         layers += [nn.Dropout(p=0.5)]        \n",
    "#         layers += [nn.Linear(10,n_gmm)]\n",
    "#         layers += [nn.Softmax(dim=1)]\n",
    "\n",
    "\n",
    "        self.estimation = estimator.net\n",
    "\n",
    "        self.register_buffer(\"phi\", torch.zeros(n_gmm))\n",
    "        self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n",
    "        self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n",
    "   \n",
    "    def relative_euclidean_distance(self, x1, x2, eps):\n",
    "        \"\"\"\n",
    "        x1 and x2 are assumed to be Variables or Tensors.\n",
    "        They have shape [batch_size, dimension_embedding]\n",
    "        相対ユークリッド距離の計算、数式は論文を参照\n",
    "        \"\"\"\n",
    "\n",
    "#         # numpy array で計算\n",
    "#         num = np.linalg.norm(x1 -x2, ord=2)\n",
    "#         denom = np.linalg.norm(x1, ord=1)\n",
    "#         return num / np.maximum(denom, eps)\n",
    "   \n",
    "        # torch で計算\n",
    "        print(f'x1 : {x1}')\n",
    "        print(f'x22 : {x2}')\n",
    "        num = torch.norm(x1 - x2, p=2, dim=1)  # dim [batch_size]\n",
    "        denom = torch.norm(x1, p=2, dim=1)  # dim [batch_size]\n",
    "        print(f'num : {num}')\n",
    "        print(f'denom : {denom}')\n",
    "        return num / torch.max(denom, eps)\n",
    "\n",
    "    def cosine_similarity(self, x1, x2, eps):\n",
    "        \"\"\"\n",
    "        x1 and x2 are assumed to be Variables or Tensors.\n",
    "        They have shape [batch_size, dimension_embedding\n",
    "        コサイン類似度の計算、数式は論文を参照\n",
    "        \"\"\"\n",
    "        \n",
    "#         # numpy arrayで計算\n",
    "#         dot_prod = np.sum(x1 * x2)\n",
    "#         dist_x1 = np.linalg.norm(x1, ord=2)\n",
    "#         dist_x2 = np.linalg.norm(x2, ord=2)\n",
    "#         return dot_prod / np.maximum(dist_x1 * dist_x2, eps)\n",
    "        \n",
    "        # torch で計算\n",
    "        dot_prod = torch.sum(x1 * x2, dim=1)  # dim [batch_size]\n",
    "        dist_x1 = torch.norm(x1, p=2, dim=1)  # dim [batch_size]\n",
    "        dist_x2 = torch.norm(x2, p=2, dim=1)  # dim [batch_size]\n",
    "        return dot_prod / torch.max(dist_x1*dist_x2, eps)\n",
    "\n",
    "\n",
    "#     def relative_euclidean_distance(self, a, b):\n",
    "#         return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "#         rec_cosine = self.cosine_similarity(x, dec, self.eps)\n",
    "        rec_cosine = F.cosine_similarity(x, dec)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x, dec, self.eps)\n",
    "\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "\n",
    "        self.phi = phi.data\n",
    "\n",
    " \n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "        \n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        \"\"\"\n",
    "        return :\n",
    "            sample_energy\n",
    "                E(z) = -log(∑ φ exp(-1/2 (z-μ)T ∑inv (z-μ)) / sqrt(|2π∑|))\n",
    "            cov_diag\n",
    "                共分散対角行列\n",
    "                \n",
    "        \"\"\"\n",
    "        if phi is None:\n",
    "            phi = to_var(self.phi)\n",
    "        if mu is None:\n",
    "            mu = to_var(self.mu)\n",
    "        if cov is None:\n",
    "            cov = to_var(self.cov)\n",
    "\n",
    "        k, D, _ = cov.size()\n",
    "\n",
    "        \n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "        \n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + to_var(torch.eye(D)*eps)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "\n",
    "            #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n",
    "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        det_cov = torch.cat(det_cov)\n",
    "        det_cov = det_cov.to(device)\n",
    "        #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "\n",
    "        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n",
    "#         sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "        sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "\n",
    "\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "\n",
    "        recon_error = torch.mean((x - x_hat) ** 2)\n",
    "\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "\n",
    "        return loss, sample_energy, recon_error, cov_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = DAGMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83216248 0.15188222 0.7761958  0.27129416 0.38971093 0.8161424\n",
      "  0.61976807 0.02061973 0.10195588 0.71028057 0.08749903 0.21319041\n",
      "  0.8045891  0.45488211 0.72080927 0.26068037 0.89419051 0.19932481\n",
      "  0.78187668 0.88199391 0.66339858 0.92554494 0.12324947 0.25364563\n",
      "  0.19261142 0.20012666 0.93893405 0.88703559 0.88357943 0.08238283\n",
      "  0.11887701 0.12406312]\n",
      " [0.96520811 0.70356366 0.67638255 0.81723369 0.44614467 0.62861295\n",
      "  0.2789295  0.71144148 0.94371077 0.75329837 0.34940377 0.38202543\n",
      "  0.89730189 0.17160973 0.76649312 0.72053461 0.44027273 0.80069073\n",
      "  0.32415927 0.99607569 0.77292314 0.87102227 0.91608273 0.34385432\n",
      "  0.69908196 0.39244787 0.74322366 0.61411631 0.49533703 0.02329461\n",
      "  0.16132873 0.1012299 ]\n",
      " [0.85246669 0.13133938 0.17539389 0.01115635 0.36012947 0.10138671\n",
      "  0.57756541 0.06700133 0.47960979 0.87739291 0.9971416  0.93099248\n",
      "  0.26314384 0.3852812  0.34733085 0.46975059 0.36806407 0.56743816\n",
      "  0.00121871 0.5367064  0.78952665 0.70760836 0.71113395 0.71043367\n",
      "  0.04780584 0.1750088  0.48910522 0.60309536 0.90124469 0.3596642\n",
      "  0.09206173 0.27817191]\n",
      " [0.17427647 0.93868127 0.15982473 0.82297418 0.45692583 0.09943898\n",
      "  0.63646959 0.95405713 0.95639852 0.76354314 0.473811   0.27659491\n",
      "  0.55945776 0.67594409 0.44457742 0.33198648 0.10907564 0.1559834\n",
      "  0.96635867 0.08953444 0.45327176 0.14718931 0.83063639 0.44419712\n",
      "  0.07665708 0.10047317 0.15755963 0.74800135 0.53702795 0.46524542\n",
      "  0.44642959 0.81952652]\n",
      " [0.68311677 0.76582404 0.21608397 0.7428469  0.65057487 0.05913583\n",
      "  0.92790909 0.88834475 0.00578094 0.20429331 0.39708404 0.14095474\n",
      "  0.0128084  0.13969692 0.58673426 0.98373626 0.28632661 0.83811776\n",
      "  0.46911674 0.76581723 0.24175814 0.49368509 0.25460625 0.25243577\n",
      "  0.26192875 0.57013023 0.92957705 0.02265064 0.4126636  0.30328236\n",
      "  0.57988159 0.68079993]]\n",
      "tensor([[0.8322, 0.1519, 0.7762, 0.2713, 0.3897, 0.8161, 0.6198, 0.0206, 0.1020,\n",
      "         0.7103, 0.0875, 0.2132, 0.8046, 0.4549, 0.7208, 0.2607, 0.8942, 0.1993,\n",
      "         0.7819, 0.8820, 0.6634, 0.9255, 0.1232, 0.2536, 0.1926, 0.2001, 0.9389,\n",
      "         0.8870, 0.8836, 0.0824, 0.1189, 0.1241],\n",
      "        [0.9652, 0.7036, 0.6764, 0.8172, 0.4461, 0.6286, 0.2789, 0.7114, 0.9437,\n",
      "         0.7533, 0.3494, 0.3820, 0.8973, 0.1716, 0.7665, 0.7205, 0.4403, 0.8007,\n",
      "         0.3242, 0.9961, 0.7729, 0.8710, 0.9161, 0.3439, 0.6991, 0.3924, 0.7432,\n",
      "         0.6141, 0.4953, 0.0233, 0.1613, 0.1012],\n",
      "        [0.8525, 0.1313, 0.1754, 0.0112, 0.3601, 0.1014, 0.5776, 0.0670, 0.4796,\n",
      "         0.8774, 0.9971, 0.9310, 0.2631, 0.3853, 0.3473, 0.4698, 0.3681, 0.5674,\n",
      "         0.0012, 0.5367, 0.7895, 0.7076, 0.7111, 0.7104, 0.0478, 0.1750, 0.4891,\n",
      "         0.6031, 0.9012, 0.3597, 0.0921, 0.2782],\n",
      "        [0.1743, 0.9387, 0.1598, 0.8230, 0.4569, 0.0994, 0.6365, 0.9541, 0.9564,\n",
      "         0.7635, 0.4738, 0.2766, 0.5595, 0.6759, 0.4446, 0.3320, 0.1091, 0.1560,\n",
      "         0.9664, 0.0895, 0.4533, 0.1472, 0.8306, 0.4442, 0.0767, 0.1005, 0.1576,\n",
      "         0.7480, 0.5370, 0.4652, 0.4464, 0.8195],\n",
      "        [0.6831, 0.7658, 0.2161, 0.7428, 0.6506, 0.0591, 0.9279, 0.8883, 0.0058,\n",
      "         0.2043, 0.3971, 0.1410, 0.0128, 0.1397, 0.5867, 0.9837, 0.2863, 0.8381,\n",
      "         0.4691, 0.7658, 0.2418, 0.4937, 0.2546, 0.2524, 0.2619, 0.5701, 0.9296,\n",
      "         0.0227, 0.4127, 0.3033, 0.5799, 0.6808]])\n"
     ]
    }
   ],
   "source": [
    "input_np = np.random.random([5, 32])\n",
    "print(input_np)\n",
    "input_ts = torch.autograd.Variable(torch.from_numpy(input_np).float())\n",
    "print(input_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc:\n",
      "tensor([[-0.1627,  0.3520],\n",
      "        [-0.2137,  0.4200],\n",
      "        [-0.2293,  0.0741],\n",
      "        [-0.1675,  0.1354],\n",
      "        [-0.2195,  0.3743]], grad_fn=<AddmmBackward>)\n",
      "dec:\n",
      "tensor([[ 3.2856e-01,  2.1979e-01,  3.1413e-01, -1.8229e-01,  5.0145e-01,\n",
      "         -2.2579e-01, -3.5307e-01, -2.5069e-01, -2.5797e-01,  5.3137e-02,\n",
      "          4.3190e-02,  6.7920e-01, -6.1335e-02,  1.4863e-02,  4.5900e-01,\n",
      "         -4.4890e-01,  1.3823e-01, -4.0124e-02,  1.0513e-01, -2.3167e-01,\n",
      "          2.6358e-01,  6.2020e-01,  2.3446e-01,  2.5876e-01,  3.7526e-01,\n",
      "         -1.3770e-01, -3.4357e-02,  1.3643e-03,  1.0844e-01,  1.6957e-01,\n",
      "          6.5526e-02,  2.4404e-01],\n",
      "        [ 3.5790e-01,  1.8431e-01,  3.1972e-01, -1.6071e-01,  5.0431e-01,\n",
      "         -2.7042e-01, -3.5724e-01, -2.4574e-01, -2.5089e-01,  4.5844e-02,\n",
      "         -7.1206e-03,  6.6128e-01, -7.9353e-02,  5.1596e-02,  4.0523e-01,\n",
      "         -4.4704e-01,  1.1339e-01, -1.3093e-02,  1.5329e-01, -2.1366e-01,\n",
      "          2.6222e-01,  6.2408e-01,  2.4008e-01,  2.5621e-01,  3.6756e-01,\n",
      "         -1.5119e-01, -1.6778e-02,  3.1049e-02,  1.0485e-01,  1.7295e-01,\n",
      "          8.4684e-02,  1.9274e-01],\n",
      "        [ 3.2144e-01,  3.4773e-01,  3.0344e-01, -2.1093e-01,  4.9404e-01,\n",
      "         -1.2197e-01, -3.8212e-01, -2.2054e-01, -3.0037e-01,  7.1364e-02,\n",
      "          1.9824e-01,  7.1262e-01, -5.4965e-02, -2.8015e-02,  6.1311e-01,\n",
      "         -4.1507e-01,  1.6245e-01, -7.9317e-02, -3.5155e-04, -2.4161e-01,\n",
      "          2.9271e-01,  6.2114e-01,  1.9502e-01,  2.3546e-01,  4.1975e-01,\n",
      "         -1.4967e-01, -1.3284e-01, -4.9417e-02,  6.1003e-02,  1.5269e-01,\n",
      "         -2.8963e-02,  3.2810e-01],\n",
      "        [ 3.0362e-01,  3.2199e-01,  3.0263e-01, -2.1631e-01,  4.9561e-01,\n",
      "         -1.3081e-01, -3.6621e-01, -2.3640e-01, -2.8752e-01,  7.0689e-02,\n",
      "          1.7115e-01,  7.1137e-01, -4.5067e-02, -3.7078e-02,  5.9023e-01,\n",
      "         -4.3110e-01,  1.7115e-01, -8.2162e-02,  7.0321e-03, -2.5019e-01,\n",
      "          2.8003e-01,  6.1787e-01,  2.0577e-01,  2.4610e-01,  4.0626e-01,\n",
      "         -1.3575e-01, -1.0534e-01, -5.1756e-02,  8.3146e-02,  1.5549e-01,\n",
      "         -4.9635e-03,  3.3119e-01],\n",
      "        [ 3.5463e-01,  2.0617e-01,  3.1827e-01, -1.6736e-01,  5.0359e-01,\n",
      "         -2.5120e-01, -3.6025e-01, -2.4243e-01, -2.5747e-01,  4.9126e-02,\n",
      "          1.9880e-02,  6.6822e-01, -7.7544e-02,  4.3101e-02,  4.3292e-01,\n",
      "         -4.4308e-01,  1.1889e-01, -2.1754e-02,  1.3445e-01, -2.1774e-01,\n",
      "          2.6622e-01,  6.2574e-01,  2.3455e-01,  2.5356e-01,  3.7523e-01,\n",
      "         -1.5236e-01, -3.2974e-02,  2.1547e-02,  9.8993e-02,  1.6978e-01,\n",
      "          6.8665e-02,  2.0888e-01]], grad_fn=<AddmmBackward>)\n",
      "z:\n",
      "tensor([[-0.1627,  0.3520,  1.0252,  0.1733],\n",
      "        [-0.2137,  0.4200,  1.0754,  0.1471],\n",
      "        [-0.2293,  0.0741,  0.9746,  0.3063],\n",
      "        [-0.1675,  0.1354,  0.9199,  0.4025],\n",
      "        [-0.2195,  0.3743,  0.9942,  0.2461]], grad_fn=<CatBackward>)\n",
      "gamma:\n",
      "tensor([[0.3649, 0.6351],\n",
      "        [0.5807, 0.4193],\n",
      "        [0.3891, 0.6109],\n",
      "        [0.5894, 0.4106],\n",
      "        [0.5307, 0.4693]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "\n",
      "enc shape:\n",
      "torch.Size([5, 2])\n",
      "dec shape:\n",
      "torch.Size([5, 32])\n",
      "z shape:\n",
      "torch.Size([5, 4])\n",
      "gamma shape:\n",
      "torch.Size([5, 2])\n",
      "\n",
      "\n",
      "enc type:\n",
      "<class 'torch.Tensor'>\n",
      "dec type:\n",
      "<class 'torch.Tensor'>\n",
      "z type:\n",
      "<class 'torch.Tensor'>\n",
      "gamma type:\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "enc, dec, z, gamma = gmm.forward(input_ts)\n",
    "print(f'enc:\\n{enc}')\n",
    "print(f'dec:\\n{dec}')\n",
    "print(f'z:\\n{z}')\n",
    "print(f'gamma:\\n{gamma}')\n",
    "print(f'\\n')\n",
    "print(f'enc shape:\\n{enc.shape}')\n",
    "print(f'dec shape:\\n{dec.shape}')\n",
    "print(f'z shape:\\n{z.shape}')\n",
    "print(f'gamma shape:\\n{gamma.shape}')\n",
    "print(f'\\n')\n",
    "print(f'enc type:\\n{type(enc)}')\n",
    "print(f'dec type:\\n{type(dec)}')\n",
    "print(f'z type:\\n{type(z)}')\n",
    "print(f'gamma type:\\n{type(gamma)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test compute_gmm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi:\n",
      "tensor([0.4910, 0.5090], grad_fn=<DivBackward0>)\n",
      "mu:\n",
      "tensor([[-0.1988,  0.2769,  0.9971,  0.2589],\n",
      "        [-0.1983,  0.2657,  0.9986,  0.2513]], grad_fn=<DivBackward0>)\n",
      "cov:\n",
      "tensor([[[ 7.2142e-04, -6.1808e-04, -5.8306e-04,  8.4065e-04],\n",
      "         [-6.1808e-04,  1.9065e-02,  6.2524e-03, -1.1416e-02],\n",
      "         [-5.8306e-04,  6.2524e-03,  3.0817e-03, -5.2522e-03],\n",
      "         [ 8.4065e-04, -1.1416e-02, -5.2522e-03,  9.3904e-03]],\n",
      "\n",
      "        [[ 8.2206e-04,  7.3147e-04, -1.5330e-04, -6.7713e-05],\n",
      "         [ 7.3147e-04,  1.9510e-02,  5.1974e-03, -1.0143e-02],\n",
      "         [-1.5330e-04,  5.1974e-03,  2.2896e-03, -4.0687e-03],\n",
      "         [-6.7713e-05, -1.0143e-02, -4.0687e-03,  7.7273e-03]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "\n",
      "\n",
      "phi shape:\n",
      "torch.Size([2])\n",
      "mu shape:\n",
      "torch.Size([2, 4])\n",
      "cov shape:\n",
      "torch.Size([2, 4, 4])\n",
      "\n",
      "\n",
      "phi type:\n",
      "<class 'torch.Tensor'>\n",
      "mu type:\n",
      "<class 'torch.Tensor'>\n",
      "cov: type\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "phi, mu, cov= gmm.compute_gmm_params(z=z, gamma=gamma)\n",
    "\n",
    "print(f'phi:\\n{phi}')\n",
    "print(f'mu:\\n{mu}')\n",
    "print(f'cov:\\n{cov}')\n",
    "print(f'\\n')\n",
    "print(f'phi shape:\\n{phi.shape}')\n",
    "print(f'mu shape:\\n{mu.shape}')\n",
    "print(f'cov shape:\\n{cov.shape}')\n",
    "print(f'\\n')\n",
    "print(f'phi type:\\n{type(phi)}')\n",
    "print(f'mu type:\\n{type(mu)}')\n",
    "print(f'cov: type\\n{type(cov)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test compute_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample energy:\n",
      "0.9110661745071411\n",
      "cov diag:\n",
      "3703.48828125\n",
      "\n",
      "\n",
      "sample energy shape:\n",
      "torch.Size([])\n",
      "cov diag shape:\n",
      "torch.Size([])\n",
      "\n",
      "\n",
      "sample energy type:\n",
      "<class 'torch.Tensor'>\n",
      "cov diag type:\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "z = to_var(z)\n",
    "energy, cov_diag = gmm.compute_energy(z=z,\n",
    "#                                       phi=phi, mu=mu, cov=cov\n",
    "                                     )\n",
    "\n",
    "print(f'sample energy:\\n{energy}')\n",
    "print(f'cov diag:\\n{cov_diag}')\n",
    "print(f'\\n')\n",
    "print(f'sample energy shape:\\n{energy.shape}')\n",
    "print(f'cov diag shape:\\n{cov_diag.shape}')\n",
    "print(f'\\n')\n",
    "print(f'sample energy type:\\n{type(energy)}')\n",
    "print(f'cov diag type:\\n{type(cov_diag)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "13715826.0\n",
      "sample energy:\n",
      "0.9110661745071411\n",
      "recon error:\n",
      "0.34675270318984985\n",
      "cov diag:\n",
      "3703.48828125\n",
      "\n",
      "\n",
      "loss shape:\n",
      "torch.Size([])\n",
      "sample energy shape:\n",
      "torch.Size([])\n",
      "recon error shape:\n",
      "torch.Size([])\n",
      "cov diag shape:\n",
      "torch.Size([])\n",
      "\n",
      "\n",
      "loss type:\n",
      "<class 'torch.Tensor'>\n",
      "sample energy type:\n",
      "<class 'torch.Tensor'>\n",
      "cov diag type:\n",
      "<class 'torch.Tensor'>\n",
      "cov diag type:\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "input_ts = to_var(input_ts)\n",
    "dec = to_var(dec)\n",
    "gamma = to_var(gamma)\n",
    "loss, sample_energy, recon_error, cov_diag = gmm.loss_function(x=input_ts, x_hat=dec, z=z, gamma=gamma, lambda_energy=energy, lambda_cov_diag=cov_diag)\n",
    "\n",
    "print(f'loss:\\n{loss}')\n",
    "print(f'sample energy:\\n{sample_energy}')\n",
    "print(f'recon error:\\n{recon_error}')\n",
    "print(f'cov diag:\\n{cov_diag}')\n",
    "print(f'\\n')\n",
    "print(f'loss shape:\\n{loss.shape}')\n",
    "print(f'sample energy shape:\\n{energy.shape}')\n",
    "print(f'recon error shape:\\n{recon_error.shape}')\n",
    "print(f'cov diag shape:\\n{cov_diag.shape}')\n",
    "print(f'\\n')\n",
    "print(f'loss type:\\n{type(loss)}')\n",
    "print(f'sample energy type:\\n{type(energy)}')\n",
    "print(f'cov diag type:\\n{type(cov_diag)}')\n",
    "print(f'cov diag type:\\n{type(cov_diag)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.FloatTensor(np.array([4]))\n",
    "x2 = torch.FloatTensor(np.array([5]))\n",
    "\n",
    "# x1 = np.array([4,5,6,7])\n",
    "# x2 = np.array([5,6,7,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DaGMM(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=10, out_features=32, bias=True)\n",
       "  )\n",
       "  (estimation): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (4): Softmax()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaGMM\n",
      "DaGMM(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=118, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=30, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=30, out_features=10, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=30, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=30, out_features=60, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=60, out_features=118, bias=True)\n",
      "  )\n",
      "  (estimation): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (4): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 18761\n"
     ]
    }
   ],
   "source": [
    "print_network(gmm, 'DaGMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network(model, name):\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(name)\n",
    "        print(model)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "        self.dagmm.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n",
    "\n",
    "        print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n",
    "\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.load_state_dict of DaGMM(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=118, out_features=60, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=60, out_features=30, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=30, out_features=10, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=10, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=30, out_features=60, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=60, out_features=118, bias=True)\n",
       "  )\n",
       "  (estimation): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (4): Softmax(dim=1)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../fastd-asd-experiments/src/utils/')\n",
    "sys.path.append('../../fastd-asd-experiments/src/')\n",
    "sys.path.append('../../fastd-asd-experiments/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import ExpUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work-hmcomm/Experiment_DataSet/feature_dataset/001_HITACHI_fan/0db/FASTD_standard/csv/001_HITACHI_fan_0db_anom_test.csv',\n",
       " '/work-hmcomm/Experiment_DataSet/feature_dataset/001_HITACHI_fan/0db/FASTD_standard/csv/001_HITACHI_fan_0db_anom_train.csv',\n",
       " '/work-hmcomm/Experiment_DataSet/feature_dataset/001_HITACHI_fan/0db/FASTD_standard/csv/001_HITACHI_fan_0db_norm_test.csv',\n",
       " '/work-hmcomm/Experiment_DataSet/feature_dataset/001_HITACHI_fan/0db/FASTD_standard/csv/001_HITACHI_fan_0db_norm_train.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util = ExpUtils.ExpUtils()\n",
    "data_set = '001_HITACHI_fan'\n",
    "anr = '0db'\n",
    "feature_type = 'FASTD_standard'\n",
    "\n",
    "dataset_path = util.find_feature_data_set(data_set, anr, feature_type)\n",
    "display(dataset_path)\n",
    "dataset = util.load_feature_data_set(dataset_path)\n",
    "norm_train = dataset[3]\n",
    "anom_train = dataset[1]\n",
    "norm_test  = dataset[2]\n",
    "anom_test  = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使わないカラムの削除\n",
    "drop_columns = ['wav_name', 'target', 'energy', 'mfcc_1']\n",
    "norm_train = norm_train.drop(columns=drop_columns)\n",
    "anom_train = anom_train.drop(columns=drop_columns)\n",
    "norm_test  = norm_test.drop(columns=drop_columns)\n",
    "anom_test  = anom_test.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31551, 32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, ):\n",
    "        # TODO\n",
    "        # 1. ファイルパスまたはファイル名のリストを初期化\n",
    "        util = ExpUtils.ExpUtils()\n",
    "        data_set = '001_HITACHI_fan'\n",
    "        anr = '0db'\n",
    "        feature_type = 'FASTD_standard'\n",
    "        dataset_path = util.find_feature_data_set(data_set, anr, feature_type)\n",
    "        self.dataset = util.load_feature_data_set(dataset_path)\n",
    "        self.drop_columns = ['wav_name', 'target', 'energy', 'mfcc_1']\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # TODO\n",
    "        # 1. ファイルから1つのデータを読み込む\n",
    "        # 2. データの前処理\n",
    "        # 3. データのペアを返す\n",
    "        norm_train = self.dataset[3].drop(columns=self.drop_columns)\n",
    "        anom_train = self.dataset[1].drop(columns=self.drop_columns)\n",
    "        norm_test  = self.dataset[2].drop(columns=self.drop_columns)\n",
    "        anom_test  = self.dataset[0].drop(columns=self.drop_columns)\n",
    "        \n",
    "        return norm_train.iloc[index,:].values, anom_train.iloc[index,:].values, norm_test.iloc[index,:].values, anom_test.iloc[index,:].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 少なくとも０を設定する\n",
    "        return len(self.dataset[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train, _, _, _  = data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.44643080e-01,  3.30201341e+00,  2.42535491e-01,  2.25260308e-01,\n",
       "        1.37212106e+00,  2.83849890e-04,  2.69000000e-01,  1.15565540e+00,\n",
       "       -9.47507137e-02,  3.98728772e-02, -1.98528710e-01,  2.49706698e-01,\n",
       "        1.62674242e-01,  8.65621306e-02,  6.64090488e-02,  8.16060156e-02,\n",
       "       -1.44304370e-01,  1.81302740e-02, -3.87194224e-02,  1.57128081e-03,\n",
       "        7.09372068e-03,  1.86572910e-02,  4.65035238e-03,  1.24156651e-01,\n",
       "        3.21724920e-03,  3.19258434e-03,  2.34249430e-03,  1.83047468e-03,\n",
       "        6.16974642e-03,  2.67911273e-03,  1.21491189e-03,  3.33033310e-02])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=data_set, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=20, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(32, 20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(10, 5)\n",
    "        ) \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(5, 10),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = Autoencoder()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/4\n",
      "torch.Size([1000, 32])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1000, 32])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1000, 32])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([198, 32])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch : {epoch}/{epochs}')\n",
    "    for i in data_loader:\n",
    "        norm = i[0]\n",
    "        print(norm.shape)\n",
    "#         norm = torch.FloatTensor(norm)\n",
    "        norm.float()\n",
    "        print(type(norm))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], loss: 0.4927\n",
      "epoch [2/5], loss: 0.4858\n",
      "epoch [3/5], loss: 0.4811\n",
      "epoch [4/5], loss: 0.4701\n",
      "epoch [5/5], loss: 0.4675\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in data_loader:\n",
    "        x_norm = data[0].float()\n",
    "#         x = img.view(img.size(0), -1)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        x_norm = x_norm.to(device) \n",
    "        x_norm = Variable(x_norm)\n",
    "        xhat_norm = model(x_norm)\n",
    "        # 再構成と入力との間でlossを計算\n",
    "        loss = criterion(xhat_norm, x_norm)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(\n",
    "        epoch + 1,\n",
    "        num_epochs,\n",
    "        loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, data_loader):\n",
    "        self.num_epoch = 10000\n",
    "        self.num_batch = 128\n",
    "        self.lr = 0.001 \n",
    "        self.data_loader = data_loader\n",
    "        \n",
    "        # build model\n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # Define model\n",
    "        self.dagmm = DAGMM()\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        # Print network\n",
    "        num_params = 0\n",
    "        for p in self.dagmm.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(self.dagmm)\n",
    "        print(f'model:name : DaGMM')\n",
    "        print(f'parameterの数 : {num_params}')\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.dagmm = self.dagmm.to(self.device)\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "        \n",
    "    def to_var(self, x, volatile=False):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        x = x.to(self.device)\n",
    "        return Variable(x, volatile=volatile)\n",
    "    \n",
    "    def train(self):\n",
    "        start = 0\n",
    "        \n",
    "        # start training \n",
    "        iter_ctr = 0\n",
    "        for e in range(start, self.num_epoch):\n",
    "            for i , input_data in enumerate(self.data_loader): \n",
    "                input_data = input_data[0].float()\n",
    "                input_data = self.to_var(input_data)\n",
    "                print(input_data)\n",
    "                \n",
    "                total_loss, sample_energy, cov_diag = self.dagmm_step(input_data)\n",
    "                \n",
    "                loss = {}\n",
    "                loss['total_loss'] = total_loss.data.item()\n",
    "                loss['sample_energy'] = sample_energy.item()\n",
    "                loss['recon_error'] = recon_error.item()\n",
    "                loss['cov_diag'] = cov_diag.item()\n",
    "                \n",
    "                print('epoch [{}/{}], loss: {:.4f}'.format(\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        loss))\n",
    "                \n",
    "    def dagmm_step(self, input_data):\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "        \n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lamda_energy)\n",
    "        self.rest_grad()\n",
    "        total_loss.backward()\n",
    "        self.nn.utils.clip_grad_norm_(self.dagmm.parameteers(), 5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAGMM(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=32, bias=True)\n",
      "  )\n",
      "  (estimation): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (4): Softmax()\n",
      "  )\n",
      ")\n",
      "model:name : DaGMM\n",
      "parameterの数 : 806\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.8507e-02, 3.2606e+00, 1.3989e-01,  ..., 1.0499e-02, 3.6066e-03,\n",
      "         2.9209e-02],\n",
      "        [8.0760e-02, 3.2631e+00, 2.4379e-01,  ..., 4.0047e-03, 1.7704e-03,\n",
      "         3.9375e-02],\n",
      "        [5.5507e-02, 3.2665e+00, 1.4651e-01,  ..., 2.1727e-03, 1.5564e-03,\n",
      "         3.7637e-02],\n",
      "        ...,\n",
      "        [1.1339e-01, 3.2938e+00, 2.2231e-01,  ..., 3.6866e-03, 4.2515e-03,\n",
      "         4.6581e-02],\n",
      "        [8.3760e-02, 3.2500e+00, 1.8968e-01,  ..., 1.2751e-02, 1.7834e-03,\n",
      "         5.1494e-02],\n",
      "        [9.8762e-02, 3.3075e+00, 2.0993e-01,  ..., 8.0711e-03, 4.0048e-03,\n",
      "         1.6999e-02]], device='cuda:0')\n",
      "x1 : tensor([[5.8507e-02, 3.2606e+00, 1.3989e-01,  ..., 1.0499e-02, 3.6066e-03,\n",
      "         2.9209e-02],\n",
      "        [8.0760e-02, 3.2631e+00, 2.4379e-01,  ..., 4.0047e-03, 1.7704e-03,\n",
      "         3.9375e-02],\n",
      "        [5.5507e-02, 3.2665e+00, 1.4651e-01,  ..., 2.1727e-03, 1.5564e-03,\n",
      "         3.7637e-02],\n",
      "        ...,\n",
      "        [1.1339e-01, 3.2938e+00, 2.2231e-01,  ..., 3.6866e-03, 4.2515e-03,\n",
      "         4.6581e-02],\n",
      "        [8.3760e-02, 3.2500e+00, 1.8968e-01,  ..., 1.2751e-02, 1.7834e-03,\n",
      "         5.1494e-02],\n",
      "        [9.8762e-02, 3.3075e+00, 2.0993e-01,  ..., 8.0711e-03, 4.0048e-03,\n",
      "         1.6999e-02]], device='cuda:0')\n",
      "x22 : tensor([[ 0.3341, -0.2261, -0.2598,  ...,  0.4591, -0.3141,  0.2903],\n",
      "        [ 0.3301, -0.2256, -0.2615,  ...,  0.4550, -0.3157,  0.2891],\n",
      "        [ 0.3338, -0.2258, -0.2606,  ...,  0.4602, -0.3163,  0.2899],\n",
      "        ...,\n",
      "        [ 0.3135, -0.2301, -0.2565,  ...,  0.4059, -0.2811,  0.2898],\n",
      "        [ 0.3233, -0.2272, -0.2600,  ...,  0.4363, -0.3030,  0.2894],\n",
      "        [ 0.3140, -0.2276, -0.2610,  ...,  0.4202, -0.2980,  0.2880]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "num : tensor([4.8356, 4.7394, 4.7969, 4.6981, 4.7420, 4.7086, 4.7437, 4.7768, 4.6755,\n",
      "        4.7577, 4.7069, 4.6718, 4.8673, 4.9345, 4.7378, 4.6271, 4.6837, 4.6620,\n",
      "        4.5709, 4.6639, 4.1863, 5.2085, 4.7842, 4.6868, 4.6526, 4.9029, 4.8660,\n",
      "        4.7616, 4.7407, 4.7592, 5.1097, 4.5599, 4.8453, 4.8185, 4.6817, 4.6254,\n",
      "        4.5673, 4.7110, 5.0071, 4.2795, 4.4903, 4.5089, 4.6513, 4.7118, 4.8773,\n",
      "        5.1158, 5.0660, 5.1551, 4.7486, 4.8213, 4.8599, 4.8519, 5.0897, 4.9437,\n",
      "        4.4650, 5.1607, 4.8041, 5.0749, 4.7286, 4.7567, 4.8124, 4.7567, 4.7200,\n",
      "        4.8177, 4.5979, 4.9787, 4.4754, 4.7581, 4.6915, 5.1104, 4.7801, 4.8829,\n",
      "        5.0249, 4.5372, 4.8879, 4.9379, 4.7249, 4.7116, 5.1022, 4.4328, 4.7666,\n",
      "        4.7891, 4.5128, 5.0852, 4.9291, 4.8676, 4.7353, 5.0378, 4.3368, 4.7226,\n",
      "        4.3449, 4.8672, 4.5560, 5.1194, 4.5103, 4.7146, 4.7863, 4.5280, 4.7922,\n",
      "        4.4114, 4.6207, 4.7886, 4.6827, 4.5714, 5.1929, 4.6047, 5.0673, 4.7110,\n",
      "        4.9563, 4.9019, 4.5775, 4.6569, 4.7181, 4.6897, 5.1311, 4.4561, 4.6679,\n",
      "        4.5451, 4.2560, 4.8084, 4.6859, 4.4023, 5.0721, 4.7014, 4.4493, 4.8828,\n",
      "        4.3768, 4.5054, 4.6983, 4.9455, 4.8204, 4.6811, 4.6524, 4.2318, 4.7102,\n",
      "        4.7321, 4.7370, 4.7323, 4.6745, 4.5496, 4.7927, 4.8448, 4.9534, 4.9259,\n",
      "        4.6928, 4.7697, 4.7601, 5.1306, 4.7587, 4.6846, 4.7505, 4.8200, 4.9726,\n",
      "        4.7290, 4.6107, 4.1888, 4.7051, 4.6790, 4.3708, 4.9710, 4.7348, 4.7514,\n",
      "        4.7953, 4.7726, 4.9406, 4.6809, 4.8428, 5.0278, 4.7876, 4.4914, 4.5769,\n",
      "        4.6592, 4.6117, 4.8160, 4.6765, 4.7474, 4.7403, 4.9652, 4.7506, 4.5283,\n",
      "        4.4328, 4.7289, 4.7197, 4.4587, 4.8467, 4.6483, 5.2302, 4.7977, 4.6775,\n",
      "        4.9374, 4.6937, 4.6264, 4.8265, 4.8153, 4.7588, 4.8635, 4.6527, 5.0859,\n",
      "        5.0844, 4.5541, 5.3752, 4.7709, 4.7765, 4.6712, 4.6809, 4.5039, 5.0430,\n",
      "        4.9954, 4.6106, 4.4587, 4.7180, 5.0000, 4.3314, 4.6987, 4.6844, 4.7928,\n",
      "        4.8192, 4.6541, 4.7402, 4.8884, 4.6382, 4.5103, 4.8999, 5.1382, 4.3590,\n",
      "        4.4787, 4.6094, 4.7591, 4.9464, 5.0172, 4.5881, 4.7952, 4.9284, 4.7752,\n",
      "        4.7169, 4.9401, 4.5011, 4.7717, 4.7200, 4.7347, 4.9785, 4.6437, 4.6965,\n",
      "        4.6653, 4.8363, 4.6246, 4.7296, 5.0086, 5.0460, 4.7378, 4.7946, 4.3666,\n",
      "        4.7565, 4.7229, 5.0786, 4.3970, 4.7147, 4.8215, 4.8588, 4.4437, 4.5572,\n",
      "        4.7802, 4.4793, 4.4217, 4.5017, 4.9660, 4.7998, 4.7582, 5.0010, 4.6840,\n",
      "        5.0118, 4.4367, 4.2030, 4.7742, 4.7681, 4.7798, 4.8857, 4.5476, 5.0744,\n",
      "        4.6336, 4.8236, 5.0395, 4.8462, 4.4611, 4.8383, 4.6807, 5.0687, 4.5949,\n",
      "        4.6887, 5.0782, 4.9303, 4.6181, 4.4936, 4.7778, 4.5251, 4.6707, 4.6309,\n",
      "        4.9063, 4.8629, 4.7771, 4.6666, 4.5877, 4.9698, 4.7843, 4.6003, 4.3777,\n",
      "        4.6866, 4.7537, 5.0553, 4.7744, 4.7990, 4.7397, 4.6684, 4.7240, 4.3191,\n",
      "        4.9485, 5.2130, 4.6610, 4.7045, 4.7098, 4.6563, 4.3489, 4.7586, 4.7510,\n",
      "        4.9274, 4.9498, 4.8196, 4.6456, 4.8008, 4.2865, 4.4922, 4.8112, 4.9771,\n",
      "        4.7798, 4.6853, 4.6865, 4.9407, 4.4377, 5.1314, 4.9763, 4.4987, 4.8260,\n",
      "        4.6885, 4.6439, 5.0186, 4.6433, 4.3766, 4.6855, 4.6432, 4.6729, 4.7217,\n",
      "        4.8518, 5.0000, 4.3051, 5.0354, 4.4310, 4.6487, 4.8257, 4.7356, 5.0314,\n",
      "        4.4926, 4.7412, 4.4965, 4.7044, 4.7596, 4.4942, 4.6839, 4.6851, 4.6127,\n",
      "        4.6075, 4.7257, 4.4745, 4.8743, 4.7412, 4.7138, 4.9044, 4.6389, 4.6757,\n",
      "        4.4993, 4.7480, 4.5899, 4.2120, 4.8530, 4.4940, 4.8339, 4.5876, 4.4186,\n",
      "        4.3812, 4.5248, 4.3674, 4.6888, 4.4667, 4.4071, 4.6308, 4.7635, 4.7383,\n",
      "        4.9264, 4.4985, 4.9288, 5.1535, 4.4565, 4.5147, 4.8171, 4.7502, 4.6195,\n",
      "        4.7058, 4.7454, 4.7597, 4.8089, 5.1015, 5.1489, 4.5317, 4.6946, 4.9431,\n",
      "        4.3931, 4.5722, 4.7601, 4.8035, 4.7884, 5.0318, 4.7540, 4.8021, 4.6340,\n",
      "        4.8560, 4.8286, 4.7177, 5.0701, 4.9979, 4.6598, 4.7236, 4.7938, 4.3213,\n",
      "        4.2563, 4.7497, 4.9581, 4.7419, 4.6304, 4.6966, 4.6352, 4.7662, 4.8134,\n",
      "        4.8375, 4.8217, 4.7735, 4.6950, 4.7499, 4.7984, 4.7124, 4.8496, 4.6367,\n",
      "        4.5692, 4.7251, 4.7059, 4.3772, 4.6327, 4.8371, 4.3171, 4.6199, 4.6809,\n",
      "        4.8005, 4.9863, 4.4971, 4.9537, 4.2516, 4.7244, 4.3184, 4.7739, 4.4332,\n",
      "        4.6154, 4.9501, 4.7761, 4.7659, 4.7616, 4.7592, 4.7753, 4.8756, 4.7071,\n",
      "        4.5587, 4.8793, 4.8428, 4.8176, 4.8245, 4.8169, 5.0579, 4.4745, 4.7224,\n",
      "        4.4555, 4.6987, 5.0329, 4.7817, 4.6447, 4.8722, 4.6977, 4.7022, 4.7537,\n",
      "        4.8815, 4.9538, 5.0149, 4.7048, 4.4949, 4.7241, 4.4692, 4.7520, 4.7169,\n",
      "        4.9731, 5.0434, 4.7289, 4.8739, 5.1236, 4.6923, 5.0246, 4.9174, 4.8388,\n",
      "        4.8613, 5.0442, 4.9948, 5.2263, 4.5185, 4.4314, 4.7180, 4.9296, 4.8670,\n",
      "        4.7583, 4.9558, 4.7762, 4.9211, 4.4716, 4.6204, 4.8548, 4.6721, 4.8751,\n",
      "        4.7538, 4.4472, 4.9816, 4.8214, 4.8947, 4.9570, 4.7580, 4.9924, 4.8178,\n",
      "        4.5415, 5.1320, 4.6747, 4.6910, 4.9697, 4.5731, 4.6957, 4.6409, 4.7603,\n",
      "        4.9691, 4.7185, 5.0384, 4.5203, 4.9850, 4.8353, 4.6767, 4.5011, 5.0969,\n",
      "        4.6801, 4.7600, 4.7369, 4.7716, 4.7879, 4.6955, 4.6887, 4.7915, 4.7432,\n",
      "        4.6928, 5.1282, 4.2352, 4.4633, 5.0589, 4.6910, 4.5179, 4.4524, 4.4514,\n",
      "        5.0089, 4.7715, 4.4088, 4.4499, 4.7076, 4.9436, 4.6896, 4.9141, 4.7064,\n",
      "        4.7291, 4.3981, 4.7932, 4.6629, 4.7577, 4.7798, 4.9227, 4.8402, 4.7625,\n",
      "        4.7125, 4.8000, 4.4453, 5.2156, 4.8670, 4.7076, 4.6444, 4.8014, 4.7990,\n",
      "        4.3041, 4.9974, 4.6195, 4.8314, 4.7610, 4.9624, 5.0236, 4.7992, 4.7717,\n",
      "        4.8484, 4.7722, 4.7590, 4.6472, 5.0333, 4.4176, 4.9602, 4.7107, 4.8376,\n",
      "        4.7204, 4.6801, 4.3493, 4.9883, 4.4353, 4.8595, 4.8013, 4.4626, 4.7415,\n",
      "        4.9582, 4.7679, 4.7161, 4.9782, 4.4826, 4.8259, 4.7944, 4.6555, 4.9002,\n",
      "        4.6972, 4.7776, 4.7290, 4.6478, 4.1728, 4.6772, 4.8025, 4.8110, 4.3827,\n",
      "        4.6019, 4.7574, 4.9751, 4.4251, 4.7427, 4.6928, 4.6822, 4.7296, 4.3815,\n",
      "        4.3662, 4.7130, 4.7087, 4.8424, 4.8764, 4.9489, 4.6663, 4.7553, 4.7970,\n",
      "        4.7449, 4.7928, 4.9804, 4.8704, 4.7013, 4.6348, 5.0144, 4.4784, 4.7489,\n",
      "        4.6822, 4.9188, 4.6791, 4.9794, 4.4923, 4.4037, 4.8252, 4.8683, 4.9721,\n",
      "        4.9958, 4.5888, 4.6041, 4.7011, 5.1182, 4.6917, 4.8053, 4.7517, 4.7714,\n",
      "        4.6868, 4.4438, 4.7439, 4.7503, 4.6876, 4.6908, 5.0061, 4.6569, 4.6027,\n",
      "        4.8924, 4.7521, 4.8097, 5.0097, 4.6151, 4.9632, 4.7724, 4.8192, 4.6592,\n",
      "        4.9628, 4.6951, 4.8545, 4.3328, 4.9257, 4.8626, 4.6950, 4.7948, 4.5291,\n",
      "        4.3748, 4.6309, 4.8530, 4.6507, 4.7888, 5.0779, 4.6605, 4.5248, 4.5416,\n",
      "        4.9166, 4.7947, 4.7586, 4.8512, 4.7295, 4.7072, 4.7414, 4.7896, 4.8353,\n",
      "        4.8570, 4.8260, 4.6884, 4.5788, 4.7625, 4.6015, 4.8384, 4.4126, 4.9868,\n",
      "        4.7109, 5.1296, 4.5886, 4.9926, 5.0331, 4.7455, 4.6094, 4.3252, 4.8582,\n",
      "        4.7060, 4.8570, 4.7855, 4.7407, 4.6483, 4.8842, 4.6625, 4.9803, 4.7316,\n",
      "        4.6492, 5.0438, 4.8046, 4.9047, 4.7288, 4.6381, 4.7177, 4.7921, 4.6791,\n",
      "        4.7297, 4.6826, 5.0004, 4.5747, 4.7113, 4.3858, 4.5135, 4.7381, 4.4781,\n",
      "        4.7764, 4.9589, 4.7040, 4.4676, 4.7944, 5.0292, 4.9641, 4.8264, 4.4935,\n",
      "        4.7090, 4.7314, 4.8066, 4.8406, 4.5680, 4.4233, 4.5779, 4.7661, 4.7296,\n",
      "        4.6529, 4.6527, 4.7004, 4.6870, 4.5113, 4.6486, 4.7421, 4.8637, 4.6465,\n",
      "        4.6506, 4.8514, 4.7245, 4.8245, 4.9387, 4.7915, 4.9273, 4.8740, 4.8002,\n",
      "        4.4291, 4.7887, 4.9230, 4.7173, 4.8233, 4.6982, 4.6858, 4.6849, 4.7495,\n",
      "        4.7381, 4.6888, 5.0149, 4.6972, 4.5188, 5.0218, 4.6315, 4.7203, 4.5193,\n",
      "        4.7832, 4.5497, 4.9337, 4.7164, 4.8573, 5.0511, 5.0166, 4.6670, 4.6218,\n",
      "        4.7826, 4.9250, 4.8715, 4.4896, 4.7567, 4.6792, 4.8265, 4.5384, 4.6913,\n",
      "        4.7917, 4.7962, 4.5647, 4.3958, 4.8830, 4.6007, 4.8288, 4.9767, 4.8122,\n",
      "        4.9117, 5.0440, 4.9091, 4.8785, 4.6387, 4.2895, 4.8963, 4.7983, 5.0818,\n",
      "        4.6715, 4.6828, 4.7759, 4.8170, 4.9622, 4.3948, 4.7128, 4.6721, 4.7090,\n",
      "        4.6093, 4.9674, 4.7221, 4.6918, 4.8702, 4.9944, 4.8311, 4.4369, 4.3771,\n",
      "        4.6961, 4.7613, 5.0107, 4.9194, 5.0226, 4.7459, 4.8285, 4.8148, 4.4801,\n",
      "        4.3646, 4.8748, 4.6237, 4.3983, 4.8223, 4.7866, 4.7415, 4.7685, 4.3531,\n",
      "        4.7608, 4.8306, 4.4647, 4.9644, 4.7131, 5.0896, 4.7543, 4.8083, 4.7146,\n",
      "        4.8674, 4.8273, 4.3911, 4.8908, 4.7467, 4.8373, 4.8110, 4.6665, 4.7964,\n",
      "        4.6611, 4.7523, 5.0881, 4.6437, 4.5099, 5.1066, 4.7476, 4.2983, 4.8132,\n",
      "        4.8607, 4.8165, 4.3036, 4.3584, 4.7414, 4.9064, 4.8310, 4.3370, 4.7128,\n",
      "        4.4884, 4.7483, 4.8795, 5.0860, 4.9513, 4.7169, 4.8256, 4.7179, 4.6469,\n",
      "        4.7646, 4.6814, 4.6709, 4.7957, 5.2012, 4.6153, 4.6862, 4.6003, 4.6567,\n",
      "        4.2989, 4.3863, 4.7330, 4.9082, 4.7251, 4.6537, 4.7443, 5.0645, 4.8835,\n",
      "        4.6823, 4.6470, 4.6992, 4.9568, 4.4048, 4.5331, 4.9936, 4.4147, 4.9095,\n",
      "        4.7698, 4.6363, 5.0980, 5.0900, 4.6707, 4.9567, 4.4369, 4.8609, 4.7439,\n",
      "        4.4261, 4.6349, 4.8889, 4.4566, 4.7793, 4.8524, 4.8687, 4.3239, 4.4709,\n",
      "        4.7152], device='cuda:0', grad_fn=<NormBackward1>)\n",
      "denom : tensor([4.3467, 4.2121, 4.2776, 4.1955, 4.2062, 4.2113, 4.2551, 4.2774, 4.2076,\n",
      "        4.2999, 4.2880, 4.1828, 4.4206, 4.4234, 4.2478, 4.0489, 4.1923, 4.1599,\n",
      "        4.0819, 4.1484, 3.7148, 4.7612, 4.2906, 4.2129, 4.1502, 4.5081, 4.3244,\n",
      "        4.3694, 4.3233, 4.2461, 4.6196, 4.0219, 4.3206, 4.2847, 4.1438, 4.1150,\n",
      "        4.0760, 4.1653, 4.5032, 3.8208, 3.9184, 3.9817, 4.0952, 4.1915, 4.3762,\n",
      "        4.6675, 4.6115, 4.6863, 4.2367, 4.3128, 4.4179, 4.4119, 4.5970, 4.4765,\n",
      "        3.9401, 4.6529, 4.3085, 4.6135, 4.2664, 4.2405, 4.3044, 4.2358, 4.2747,\n",
      "        4.3006, 4.0134, 4.4533, 3.9797, 4.2539, 4.1945, 4.5947, 4.2720, 4.3625,\n",
      "        4.5137, 4.0189, 4.4012, 4.4836, 4.3033, 4.2827, 4.6232, 3.9139, 4.2603,\n",
      "        4.2901, 4.0219, 4.5547, 4.5128, 4.3687, 4.3004, 4.5534, 3.8118, 4.2152,\n",
      "        3.8062, 4.3531, 4.0606, 4.6380, 3.9889, 4.2316, 4.3094, 3.9490, 4.2689,\n",
      "        3.8635, 4.1087, 4.2768, 4.2693, 4.1392, 4.7453, 4.1158, 4.5814, 4.2875,\n",
      "        4.4853, 4.4190, 4.0045, 4.2193, 4.1955, 4.3157, 4.6668, 3.9545, 4.2141,\n",
      "        3.9640, 3.7945, 4.3094, 4.2426, 3.9406, 4.6079, 4.2077, 3.9709, 4.4762,\n",
      "        3.9107, 3.9883, 4.2841, 4.4506, 4.3159, 4.2448, 4.2607, 3.7854, 4.2698,\n",
      "        4.2150, 4.2288, 4.3222, 4.2576, 4.0273, 4.2708, 4.3218, 4.5094, 4.4924,\n",
      "        4.2037, 4.3577, 4.2383, 4.7088, 4.2805, 4.1870, 4.2940, 4.3179, 4.4904,\n",
      "        4.2683, 4.2034, 3.7354, 4.2958, 4.1920, 3.9076, 4.4473, 4.2581, 4.2582,\n",
      "        4.2582, 4.2606, 4.4485, 4.1914, 4.3546, 4.5227, 4.3039, 3.9986, 3.9898,\n",
      "        4.1316, 4.1108, 4.4063, 4.1985, 4.2534, 4.2338, 4.4501, 4.2170, 3.9927,\n",
      "        3.8762, 4.3243, 4.2131, 4.0119, 4.3668, 4.1466, 4.7814, 4.3469, 4.2511,\n",
      "        4.4156, 4.1923, 4.1281, 4.3327, 4.3437, 4.2189, 4.3807, 4.1455, 4.6096,\n",
      "        4.6025, 4.0607, 4.8717, 4.2690, 4.3812, 4.1839, 4.2730, 3.9983, 4.5392,\n",
      "        4.5439, 4.2211, 3.9611, 4.3079, 4.4581, 3.8860, 4.2088, 4.2178, 4.3074,\n",
      "        4.3765, 4.1507, 4.2886, 4.4092, 4.1358, 3.9889, 4.3900, 4.7395, 3.9056,\n",
      "        3.9689, 4.1094, 4.3023, 4.4274, 4.5183, 4.0240, 4.2456, 4.4473, 4.3324,\n",
      "        4.2231, 4.4530, 3.9528, 4.2713, 4.2975, 4.2490, 4.4692, 4.1763, 4.2082,\n",
      "        4.2594, 4.3390, 4.1599, 4.2297, 4.4918, 4.5636, 4.2362, 4.3039, 3.8913,\n",
      "        4.3488, 4.1943, 4.6251, 3.8945, 4.1993, 4.3066, 4.4084, 3.9555, 3.9922,\n",
      "        4.3032, 3.9652, 3.9384, 4.0425, 4.4301, 4.3857, 4.2758, 4.5299, 4.2858,\n",
      "        4.4775, 3.8744, 3.7249, 4.3882, 4.3772, 4.2525, 4.3800, 3.9944, 4.5892,\n",
      "        4.0991, 4.2818, 4.5759, 4.3541, 3.9920, 4.4285, 4.1919, 4.5894, 4.1540,\n",
      "        4.1716, 4.5696, 4.4111, 4.0973, 4.0271, 4.3910, 4.0021, 4.1683, 4.2194,\n",
      "        4.4191, 4.4046, 4.3568, 4.1607, 4.1951, 4.4998, 4.3067, 4.1287, 3.8926,\n",
      "        4.2049, 4.2374, 4.5452, 4.2879, 4.3147, 4.2485, 4.1608, 4.2586, 3.8542,\n",
      "        4.4376, 4.7441, 4.2451, 4.2893, 4.2823, 4.1659, 3.9260, 4.3094, 4.2321,\n",
      "        4.4642, 4.4453, 4.3274, 4.1432, 4.3054, 3.9233, 4.0025, 4.3995, 4.5214,\n",
      "        4.2985, 4.2569, 4.2801, 4.4224, 3.9292, 4.6693, 4.4385, 3.9907, 4.3451,\n",
      "        4.2917, 4.1396, 4.5326, 4.1316, 3.9179, 4.2394, 4.1437, 4.1823, 4.2256,\n",
      "        4.3671, 4.4891, 3.8719, 4.5227, 3.9209, 4.1867, 4.3244, 4.2595, 4.5626,\n",
      "        3.9349, 4.3317, 4.0050, 4.2790, 4.2633, 3.9746, 4.2221, 4.2772, 4.0846,\n",
      "        4.1472, 4.3295, 3.8979, 4.3465, 4.2366, 4.2310, 4.4223, 4.1501, 4.1614,\n",
      "        3.9741, 4.2280, 4.2010, 3.7402, 4.3468, 3.9301, 4.3290, 4.0905, 3.9135,\n",
      "        3.9359, 3.9928, 3.9175, 4.2364, 3.9680, 3.9419, 4.1338, 4.3550, 4.3315,\n",
      "        4.4414, 3.9462, 4.3935, 4.6226, 3.9618, 3.9732, 4.3459, 4.3576, 4.1299,\n",
      "        4.1818, 4.3399, 4.2761, 4.2847, 4.6057, 4.6528, 3.9265, 4.1988, 4.4854,\n",
      "        3.9079, 4.0085, 4.3597, 4.3199, 4.2767, 4.5021, 4.3162, 4.2749, 4.1550,\n",
      "        4.4286, 4.3313, 4.3231, 4.5176, 4.4944, 4.2314, 4.2605, 4.2620, 3.8358,\n",
      "        3.8088, 4.3097, 4.4524, 4.3359, 4.1232, 4.2909, 4.2631, 4.3364, 4.3031,\n",
      "        4.3232, 4.2844, 4.3138, 4.2293, 4.3547, 4.2947, 4.1918, 4.4022, 4.1703,\n",
      "        4.0569, 4.2922, 4.2111, 3.8761, 4.1257, 4.3174, 3.7826, 4.1340, 4.1921,\n",
      "        4.3005, 4.4814, 3.9984, 4.4154, 3.7301, 4.3005, 3.8323, 4.2452, 3.8974,\n",
      "        4.0987, 4.4712, 4.2744, 4.3171, 4.2736, 4.2242, 4.3627, 4.4625, 4.2693,\n",
      "        3.9727, 4.3817, 4.3631, 4.3087, 4.3289, 4.3930, 4.5577, 3.9162, 4.2355,\n",
      "        3.9479, 4.1783, 4.4991, 4.3887, 4.2544, 4.4136, 4.3121, 4.1689, 4.2374,\n",
      "        4.4335, 4.4630, 4.4860, 4.2915, 3.9720, 4.2253, 3.9843, 4.2093, 4.2954,\n",
      "        4.4808, 4.5127, 4.2983, 4.4320, 4.6151, 4.2079, 4.4927, 4.4060, 4.3223,\n",
      "        4.3428, 4.5334, 4.4844, 4.7078, 3.9413, 3.9493, 4.3173, 4.3913, 4.3635,\n",
      "        4.3013, 4.4663, 4.3470, 4.4037, 3.9610, 4.1068, 4.4361, 4.2624, 4.3503,\n",
      "        4.3333, 3.9021, 4.4441, 4.3483, 4.3674, 4.4291, 4.2627, 4.4798, 4.3693,\n",
      "        4.0155, 4.6650, 4.2848, 4.2887, 4.4397, 4.1995, 4.1899, 4.1597, 4.3344,\n",
      "        4.4718, 4.3082, 4.5685, 4.0166, 4.4984, 4.4176, 4.1619, 3.9652, 4.5800,\n",
      "        4.1372, 4.2833, 4.2536, 4.2524, 4.4009, 4.2913, 4.1688, 4.2602, 4.2279,\n",
      "        4.1855, 4.6268, 3.8147, 3.9728, 4.5955, 4.1738, 3.9264, 3.8892, 3.9197,\n",
      "        4.5081, 4.2484, 3.9338, 3.9509, 4.2061, 4.3804, 4.2211, 4.4705, 4.2106,\n",
      "        4.2650, 3.8740, 4.3087, 4.1805, 4.3005, 4.3246, 4.4641, 4.3966, 4.2939,\n",
      "        4.1573, 4.2927, 3.9595, 4.7618, 4.4299, 4.1948, 4.1287, 4.2683, 4.3026,\n",
      "        3.8196, 4.5072, 4.1553, 4.4038, 4.2431, 4.4299, 4.4918, 4.3658, 4.2790,\n",
      "        4.4264, 4.3422, 4.2783, 4.1450, 4.5158, 3.9167, 4.4334, 4.2230, 4.3568,\n",
      "        4.2151, 4.2631, 3.8443, 4.4816, 3.8752, 4.3847, 4.3471, 3.9383, 4.2554,\n",
      "        4.4526, 4.2354, 4.3088, 4.4518, 3.9210, 4.3858, 4.2576, 4.2578, 4.3949,\n",
      "        4.2034, 4.2704, 4.3388, 4.1244, 3.6658, 4.2329, 4.3247, 4.3976, 3.9437,\n",
      "        4.1087, 4.2245, 4.5025, 3.9360, 4.3317, 4.2158, 4.2196, 4.2160, 3.8844,\n",
      "        3.8850, 4.2299, 4.2203, 4.3669, 4.4054, 4.4275, 4.2010, 4.3253, 4.3058,\n",
      "        4.3125, 4.2690, 4.5384, 4.3578, 4.2324, 4.1394, 4.4984, 3.9424, 4.3205,\n",
      "        4.1825, 4.3890, 4.1899, 4.5199, 3.9065, 3.8822, 4.3481, 4.3944, 4.4886,\n",
      "        4.5001, 4.1142, 4.1402, 4.2065, 4.7228, 4.1841, 4.3736, 4.2433, 4.3393,\n",
      "        4.1841, 3.9368, 4.2228, 4.2159, 4.2378, 4.2099, 4.5208, 4.1111, 4.2050,\n",
      "        4.3947, 4.3179, 4.3393, 4.4963, 4.2068, 4.4404, 4.2841, 4.2972, 4.1663,\n",
      "        4.4399, 4.1965, 4.3967, 3.8408, 4.4364, 4.3594, 4.2473, 4.2925, 3.9807,\n",
      "        3.9000, 4.1302, 4.4154, 4.2463, 4.3086, 4.5445, 4.2213, 3.9521, 4.1411,\n",
      "        4.3946, 4.2802, 4.3759, 4.3754, 4.2185, 4.2286, 4.3333, 4.3656, 4.3193,\n",
      "        4.4782, 4.3014, 4.2799, 4.1367, 4.2141, 4.1034, 4.3249, 3.9919, 4.4787,\n",
      "        4.2077, 4.6416, 4.1680, 4.5070, 4.5769, 4.2367, 4.1204, 3.8589, 4.3740,\n",
      "        4.1972, 4.3258, 4.2971, 4.2558, 4.1575, 4.3340, 4.1558, 4.4577, 4.2376,\n",
      "        4.1599, 4.5095, 4.3020, 4.4178, 4.3261, 4.1154, 4.2579, 4.3948, 4.1684,\n",
      "        4.2146, 4.1872, 4.4954, 4.0704, 4.2249, 3.9066, 3.9439, 4.3149, 3.9705,\n",
      "        4.2743, 4.4448, 4.3075, 3.9507, 4.2977, 4.5605, 4.4786, 4.3670, 3.9668,\n",
      "        4.2368, 4.2138, 4.3443, 4.3664, 4.0326, 3.9115, 4.0285, 4.3160, 4.2232,\n",
      "        4.1640, 4.1014, 4.1855, 4.1808, 4.0129, 4.1551, 4.2555, 4.3435, 4.1378,\n",
      "        4.1531, 4.3603, 4.3206, 4.3115, 4.4500, 4.2909, 4.4049, 4.3859, 4.3887,\n",
      "        3.9270, 4.3190, 4.4105, 4.2355, 4.3006, 4.2222, 4.1940, 4.2555, 4.2344,\n",
      "        4.3276, 4.1861, 4.5045, 4.1895, 4.0109, 4.5368, 4.1766, 4.2622, 3.9953,\n",
      "        4.2249, 4.0412, 4.4533, 4.2725, 4.3093, 4.6028, 4.5634, 4.2414, 4.2116,\n",
      "        4.3473, 4.5173, 4.3668, 3.9222, 4.2416, 4.2117, 4.3253, 3.9681, 4.1997,\n",
      "        4.2486, 4.3582, 3.9917, 3.8950, 4.3649, 4.0213, 4.4226, 4.4658, 4.3216,\n",
      "        4.4155, 4.5283, 4.3835, 4.3393, 4.1327, 3.8475, 4.3890, 4.3310, 4.6415,\n",
      "        4.2605, 4.2806, 4.2897, 4.2819, 4.4082, 3.8803, 4.2741, 4.2179, 4.2352,\n",
      "        4.0306, 4.4624, 4.2108, 4.1976, 4.3434, 4.5187, 4.3789, 3.8990, 3.9501,\n",
      "        4.2499, 4.2660, 4.5304, 4.4226, 4.5268, 4.2724, 4.3655, 4.3984, 3.9768,\n",
      "        3.8764, 4.3831, 4.2261, 3.9053, 4.3773, 4.3669, 4.3311, 4.3274, 3.8877,\n",
      "        4.3037, 4.2901, 3.9378, 4.4837, 4.2065, 4.6243, 4.3252, 4.3078, 4.3043,\n",
      "        4.3568, 4.3873, 3.8369, 4.4221, 4.2969, 4.3559, 4.2838, 4.1454, 4.3159,\n",
      "        4.2356, 4.3492, 4.5888, 4.1905, 4.0138, 4.6054, 4.3190, 3.8368, 4.4145,\n",
      "        4.4685, 4.3490, 3.8492, 3.8551, 4.3327, 4.5320, 4.3419, 3.8842, 4.2025,\n",
      "        3.9706, 4.2499, 4.3797, 4.5978, 4.4460, 4.2254, 4.4112, 4.2154, 4.2314,\n",
      "        4.2580, 4.2060, 4.1091, 4.3792, 4.7599, 4.1141, 4.2245, 4.0906, 4.1875,\n",
      "        3.8082, 3.9058, 4.2277, 4.3841, 4.2368, 4.2395, 4.2515, 4.5996, 4.3692,\n",
      "        4.2583, 4.2148, 4.2932, 4.5361, 3.9093, 4.0583, 4.4791, 3.9140, 4.3974,\n",
      "        4.2684, 4.2313, 4.5840, 4.5923, 4.1748, 4.4394, 3.9904, 4.3615, 4.3501,\n",
      "        3.9150, 4.1535, 4.4907, 3.8946, 4.2673, 4.3443, 4.3778, 3.8681, 3.9352,\n",
      "        4.2434], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-ad90b845b08b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-328-f1d862d37465>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-328-f1d862d37465>\u001b[0m in \u001b[0;36mdagmm_step\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdagmm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlamda_energy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-327-82a2018b399e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;31m#         rec_cosine = self.cosine_similarity(x, dec, self.eps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mrec_cosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mrec_euclidean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_euclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_euclidean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_cosine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-327-82a2018b399e>\u001b[0m in \u001b[0;36mrelative_euclidean_distance\u001b[0;34m(self, x1, x2, eps)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'num : {num}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'denom : {denom}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagmm = DaGMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagmm.loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaGMM(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=274, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=274, bias=True)\n",
      "  )\n",
      "  (estimation): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (4): Softmax()\n",
      "  )\n",
      ")\n",
      "model:name : DaGMM\n",
      "parameterの数 : 5888\n"
     ]
    }
   ],
   "source": [
    "solver.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.reset_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5525, 0.0565, 0.2837,  ..., 0.2132, 0.7036, 0.2365],\n",
       "        [0.3383, 0.3235, 0.2650,  ..., 0.5296, 0.7910, 0.5985],\n",
       "        [0.2078, 0.6065, 0.8221,  ..., 0.7976, 0.5674, 0.0793],\n",
       "        [0.3996, 0.3767, 0.5216,  ..., 0.1398, 0.5509, 0.0792],\n",
       "        [0.8228, 0.2886, 0.9797,  ..., 0.0307, 0.1025, 0.1172]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = solver.to_cuda(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5525, 0.0565, 0.2837,  ..., 0.2132, 0.7036, 0.2365],\n",
       "        [0.3383, 0.3235, 0.2650,  ..., 0.5296, 0.7910, 0.5985],\n",
       "        [0.2078, 0.6065, 0.8221,  ..., 0.7976, 0.5674, 0.0793],\n",
       "        [0.3996, 0.3767, 0.5216,  ..., 0.1398, 0.5509, 0.0792],\n",
       "        [0.8228, 0.2886, 0.9797,  ..., 0.0307, 0.1025, 0.1172]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
